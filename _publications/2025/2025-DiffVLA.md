---
title:          "[Workshop Challenge] DiffVLA: Vision-Language Guided Diffusion Planning for Autonomous Driving"
date:           2025-05-12 00:01:00 +0800
selected:       true
pub:            "CVPR Workshop Challenge"
pub_date:       "2025"
category:       "E2E Autonomous Driving, VLA"
abstract: >-
    Research interest in end-to-end autonomous driving has surged owing to its fully differentiable design integrating modular tasks, i.e. perception, prediction and planing, which enables optimization in pursuit of the ultimate goal. Despite the great potential of the end-to-end paradigm, existing methods suffer from several aspects including expensive BEV (bird's eye view) computation, action diversity, and sub-optimal decision in complex real-world scenarios. To address these challenges, we propose a novel hybrid sparse-dense diffusion policy, empowered by a Vision-Language Model (VLM), called Diff-VLA. We explore the sparse diffusion representation for efficient multi-modal driving behavior. Moreover, we rethink the effectiveness of VLM driving decision and improve the trajectory generation guidance through deep interaction across agent, map instances and VLM output. Our method shows superior performance in Autonomous Grand Challenge 2025 which contains challenging real and reactive synthetic scenarios. Our methods achieves 45.0 PDMS.


cover: /assets/images/research/2025-diffvla/01_framework_overview.png
authors:
- Anqing Jiang*
- Yu Gao
- Zhigang Sun
- Yiru Wang
- Jijun Wang
- Jinhao Chai
- Qian Cao
- Yuwen Heng
- Hao Jiang
- Yunda Dong
- Zongzheng Zhang
- Xianda Guo
- Hao Sun
- Hao Zhao
links:
  Project Page: https://diffvla.github.io/
  Paper: https://arxiv.org/abs/2505.19381
  Code: https://github.com/DiffVLA
  
#Unsplash: https://unsplash.com/photos/sliced-in-half-pineapple--_PLJZmHZzk

---